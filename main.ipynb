{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Content-Aware Video Cropping**\n",
        "\n",
        "Content-aware video cropping solution using the YOLOv8 object detection model. It processes landscape videos, crops them to a specified aspect ratio (e.g., portrait), and ensures that the key objects or actions remain in focus. The output video retains the original audio for seamless integration.\n",
        "\n",
        "-- Also tried RCNN (Currenlty not in these code) for better accuracy but it speed is slow that's why I procced with YOLOv8\n",
        "\n",
        "RCNN - Better accuracy , slow\n",
        "\n",
        "YOLOv8 - less accuracy Compare to RCNN , Faster"
      ],
      "metadata": {
        "id": "HtWD46s9Qskn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "kX8uYobS7rVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "import numpy as np\n",
        "import subprocess\n",
        "import os"
      ],
      "metadata": {
        "id": "m1apzNEJ7ckl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: code for drive mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yz7vhhR-6TGD",
        "outputId": "02d6b833-df74-400d-b481-6cf80608f144"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loads the YOLOv8 model for object detection.**"
      ],
      "metadata": {
        "id": "am-3nr8WHsJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_yolov8_model(model_path):\n",
        "    \"\"\"\n",
        "    Load the YOLOv8 model.\n",
        "    \"\"\"\n",
        "    model = YOLO(model_path)\n",
        "    return model"
      ],
      "metadata": {
        "id": "yP4TprEiGfS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Detects objects in a video frame using the YOLOv8 model.**"
      ],
      "metadata": {
        "id": "SiSSv5sSHxYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_objects_yolov8(frame, model):\n",
        "    \"\"\"\n",
        "    Detect objects in a frame using YOLOv8.\n",
        "    \"\"\"\n",
        "    # Extract bounding box details\n",
        "    results = model(frame, stream=False)\n",
        "    detections = []\n",
        "\n",
        "    for box in results[0].boxes:\n",
        "        x_min, y_min, x_max, y_max = map(int, box.xyxy[0].tolist())\n",
        "        confidence = float(box.conf[0])\n",
        "        class_id = int(box.cls[0])\n",
        "        detections.append({\n",
        "            'x_min': x_min, 'y_min': y_min,\n",
        "            'x_max': x_max, 'y_max': y_max,\n",
        "            'confidence': confidence, 'class_id': class_id\n",
        "        })\n",
        "\n",
        "    return detections"
      ],
      "metadata": {
        "id": "56Y1GoxhGoJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Selects the most relevant object based on confidence, size, and proximity to the frame center.**"
      ],
      "metadata": {
        "id": "6z5SisCdH_iF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prioritize_objects(detections, frame_width, frame_height):\n",
        "    \"\"\"\n",
        "    Prioritize objects based on confidence, size, and proximity to the center.\n",
        "    \"\"\"\n",
        "\n",
        "    if not detections:\n",
        "        return None\n",
        "    frame_center = (frame_width / 2, frame_height / 2)\n",
        "\n",
        "    def calculate_score(roi):\n",
        "        x_center = (roi['x_min'] + roi['x_max']) / 2\n",
        "        y_center = (roi['y_min'] + roi['y_max']) / 2\n",
        "        width = roi['x_max'] - roi['x_min']\n",
        "        height = roi['y_max'] - roi['y_min']\n",
        "        area = width * height\n",
        "        distance_to_center = ((x_center - frame_center[0]) ** 2 + (y_center - frame_center[1]) ** 2) ** 0.5\n",
        "        return roi['confidence'] + 0.5 * area - 0.2 * distance_to_center\n",
        "\n",
        "    return max(detections, key=calculate_score, default=None)"
      ],
      "metadata": {
        "id": "_OvuKDfAGvBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculates a cropping box that matches the desired aspect ratio.**"
      ],
      "metadata": {
        "id": "uArn48APIDIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_to_aspect_ratio(x_center, y_center, target_aspect_ratio, frame_width, frame_height):\n",
        "    \"\"\"\n",
        "    Adjusts a cropping region to match the desired aspect ratio.\n",
        "    \"\"\"\n",
        "    if frame_height * target_aspect_ratio <= frame_width:\n",
        "        crop_height = frame_height\n",
        "        crop_width = int(crop_height * target_aspect_ratio)\n",
        "    else:\n",
        "        crop_width = frame_width\n",
        "        crop_height = int(crop_width / target_aspect_ratio)\n",
        "\n",
        "    x1 = max(0, int(x_center - crop_width / 2))\n",
        "    x2 = min(frame_width, int(x_center + crop_width / 2))\n",
        "    y1 = max(0, int(y_center - crop_height / 2))\n",
        "    y2 = min(frame_height, int(y_center + crop_height / 2))\n",
        "\n",
        "    return x1, y1, x2, y2"
      ],
      "metadata": {
        "id": "nHmdUALWGxhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Applies content-aware cropping to a video while maintaining temporal smoothing.**"
      ],
      "metadata": {
        "id": "8VcAoaRXIJHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_video(input_path, output_path, model, target_aspect_ratio=9/16, smoothing_factor=0.9):\n",
        "    \"\"\"\n",
        "    Process a video to perform content-aware cropping with temporal smoothing.\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(input_path)\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Unable to open video file.\")\n",
        "        return\n",
        "\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    output_width = int(frame_height * target_aspect_ratio)\n",
        "    output_height = frame_height\n",
        "\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (output_width, output_height))\n",
        "\n",
        "    prev_crop = None\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        detections = detect_objects_yolov8(frame, model)\n",
        "        main_object = prioritize_objects(detections, frame_width, frame_height)\n",
        "\n",
        "        if main_object:\n",
        "            x_center = (main_object['x_min'] + main_object['x_max']) / 2\n",
        "            y_center = (main_object['y_min'] + main_object['y_max']) / 2\n",
        "            x1, y1, x2, y2 = adjust_to_aspect_ratio(x_center, y_center, target_aspect_ratio, frame_width, frame_height)\n",
        "        else:\n",
        "            x1, y1, x2, y2 = adjust_to_aspect_ratio(frame_width // 2, frame_height // 2, target_aspect_ratio, frame_width, frame_height)\n",
        "\n",
        "        # smoothing\n",
        "        if prev_crop is not None:\n",
        "            x1 = int(smoothing_factor * prev_crop[0] + (1 - smoothing_factor) * x1)\n",
        "            y1 = int(smoothing_factor * prev_crop[1] + (1 - smoothing_factor) * y1)\n",
        "            x2 = int(smoothing_factor * prev_crop[2] + (1 - smoothing_factor) * x2)\n",
        "            y2 = int(smoothing_factor * prev_crop[3] + (1 - smoothing_factor) * y2)\n",
        "\n",
        "        prev_crop = (x1, y1, x2, y2)\n",
        "\n",
        "        cropped_frame = frame[y1:y2, x1:x2]\n",
        "        cropped_frame = cv2.resize(cropped_frame, (output_width, output_height))\n",
        "        out.write(cropped_frame)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    print(f\"Processed video saved to {output_path}\")"
      ],
      "metadata": {
        "id": "21kUAePJG0Ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Combines audio from the original video with the processed video.**"
      ],
      "metadata": {
        "id": "vXrMokrvIPoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_audio_to_video(input_video_path, processed_video_path, output_with_audio_path):\n",
        "    \"\"\"\n",
        "    Add audio from the input video to the processed video using FFmpeg.\n",
        "    \"\"\"\n",
        "\n",
        "    command = [\n",
        "        \"ffmpeg\", \"-i\", processed_video_path, \"-i\", input_video_path, \"-c:v\", \"copy\",\n",
        "        \"-map\", \"0:v:0\", \"-map\", \"1:a:0\", \"-y\", output_with_audio_path\n",
        "    ]\n",
        "\n",
        "    subprocess.run(command, check=True)\n",
        "    print(f\"Video with audio saved to {output_with_audio_path}\")"
      ],
      "metadata": {
        "id": "O0y5kCOVG3AK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    input_video_path = \"/content/drive/MyDrive/Demo/Input/test1.mp4\"\n",
        "    processed_video_path = \"/content/drive/MyDrive/Demo/output/output1.mp4\"\n",
        "    final_output_path = \"/content/drive/MyDrive/Demo/output/foutput1.mp4\"\n",
        "    yolo_model_path = \"yolov8n.pt\"\n",
        "\n",
        "    # Load YOLOv8 model\n",
        "    model = load_yolov8_model(yolo_model_path)\n",
        "\n",
        "    # Process the video\n",
        "    process_video(input_video_path, processed_video_path, model)\n",
        "\n",
        "    # Add audio to the processed video\n",
        "    add_audio_to_video(input_video_path, processed_video_path, final_output_path)"
      ],
      "metadata": {
        "id": "TfFEUgHXB78m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sto2mfFJ9qLl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}